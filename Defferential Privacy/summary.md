
# Gomez et al. - 2025 - $(varepsilon, δ)$ Considered Harmful Best Practices for Reporting Differential Privacy Guarantees

## 概要

この論文は、機械学習(ML)アルゴリズムにおける差分プライバシー(DP)保証の現在の報告方法が、不完全かつ誤解を招く可能性があると主張しています。著者らは、MLにおけるDP保証を伝達する主要な手段として、ガウス差分プライバシー(GDP)を使用することを提案しています。

### 問題点

- 現在の(ε, δ)-DPを用いた報告方法は、特にδの選択が任意であるため、異なる設定間でのプライバシーレベルの比較が困難である。
- (ε, δ)-DPは、多くの実用的なMLアルゴリズムの実際のプライバシー保証を正確に表現できず、リスクを過大評価する可能性がある。

### 提案

- DP保証を報告するための主要な手段として、単一のパラメータμのみを持つGDPを使用することを提案。これにより、保証の比較が容易になる。
- GDPが不正確な場合は、セカンダリな選択肢として完全なプライバシープロファイルを使用する。
- 州国勢調査局のTopDownアルゴリズムなど、最先端のDP大規模画像分類のプライバシープロファイルを調査し、GDPがこれらのプロファイルを非常によく適合させることを示している。

### 結論

GDPは最終的な保証を報告するのに理想的であるが、正確なプライバシー会計のためには他の形式(プライバシー損失確率変数など)が必要である。このような中間表現は、タイトさをほとんど失うことなく効率的にGDPに変換できることを示している。
